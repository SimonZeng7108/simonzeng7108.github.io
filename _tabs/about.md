---
# the default layout is 'page'
icon: fas fa-info-circle
order: 4
---

![Desktop View](/images/portfolio/simon_conference.jpeg){: width="800" height="800" .w-50 .right}
I am a Postdoc Research Associate at [*Visual Information Lab (VIL)*](https://vilab.blogs.bristol.ac.uk/), Bristol, supervised by [*Dr Aaron Zhang*](https://fan-aaron-zhang.github.io/) and [*Prof. David Bull*](https://www.bristol.ac.uk/people/person/David-Bull-f53987d8-4d62-431b-8228-2d39f944fbfe/). My project is funded by Prime Video Amazon on an exciting deep video quality assessment project.
Prior than this, I am a PhD research student at [*Machine Learning and Computer Vision Research Group (MaVi)*](https://uob-mavi.github.io/people/) and [*Visual Information Lab (VIL)*](https://vilab.blogs.bristol.ac.uk/),
University of Bristol, where I am supervised by [*Dr Alberto Gambaruto*](https://research-information.bris.ac.uk/en/persons/alberto-m-gambaruto) and [*Dr Tilo Burghardt*](http://people.cs.bris.ac.uk/~burghard/). We work on the swallowing project [CTAR-SwiFt](https://www.dev.fundingawards.nihr.ac.uk/award/PB-PG-1217-20005) funded by NIHR. <br/>
<!-- My work focuses on Medical Image Processing using Deep Learning and Computational Fluid Dynamics using Meshless method/Physics-Informed Neural Network. -->
<br/>
<br/>
<br/>
<br/>
<br/>

<style>
.news-scroll {
  max-height: 600px;
  overflow-y: auto;
  padding-right: 8px;
  margin-bottom: 24px;
}
.news-scroll::-webkit-scrollbar {
  width: 8px;
  background: #f0f0f0;
}
.news-scroll::-webkit-scrollbar-thumb {
  background: #c1c1c1;
  border-radius: 4px;
}
.news-scroll::-webkit-scrollbar-thumb:hover {
  background: #a8a8a8;
}
.news-scroll {
  scrollbar-width: thin;
  scrollbar-color: #c1c1c1 #f0f0f0;
}
.news-box {
  border: 1px solid #e0e4ea;
  border-radius: 14px;
  padding: 22px 24px;
  margin-bottom: 20px;
  background: linear-gradient(135deg, #fdfefe 0%, #f7fafd 100%);
  box-shadow: 0 2px 8px 0 rgba(60, 80, 120, 0.07);
  transition: box-shadow 0.2s, border-color 0.2s;
}
.news-box:hover {
  box-shadow: 0 4px 16px 0 rgba(60, 80, 120, 0.13);
  border-color: #b3c0d1;
}
.news-box img {
  display: block;
  margin: 18px auto 0 auto;
  border-radius: 8px;
  box-shadow: 0 1px 4px 0 rgba(60,80,120,0.08);
}
</style>

---
## News
---
<div class="news-scroll">

<div class="news-box">
  <span style="font-weight:600;">02/06/2025:</span> I co-organised a Richard Gregory Memorial Lecture given by
  <a href="https://www.nobelprize.org/prizes/physics/2024/hinton/facts/" target="_blank">Professor Geoffrey Hinton</a>
  at the University of Bristol.
  <div><img src="/images/portfolio/Hinton.jpg" width="300"></div>
</div>

<div class="news-box">
  <span style="font-weight:600;">20/05/2025:</span> "Multi-Teacher Knowledge Distillation for Efficient Object Segmentation" was <b>accepted</b> by the IEEE International Conference on Image Processing (<b>ICIP</b>) 2025.
</div>

<div class="news-box">
  <span style="font-weight:600;">01/04/2025:</span> "Agglomerating Large Vision Encoders via Distillation for VFSS Segmentation" was <b>accepted</b> by <b>CVPR</b> 2025, Efficient Large Vision Models (<b>ELVM</b>) Workshop (2nd Edition).
</div>

<div class="news-box">
  <span style="font-weight:600;">03/02/2025:</span> I am happy to have started a new role as a Postdoctoral Research Associate at <b>Bristol</b>, funded by Prime Video, <b>Amazon</b>.
</div>

<div class="news-box">
  <span style="font-weight:600;">17/06/2024:</span> I began a six-month internship as an Applied Scientist at <b>Amazon</b>, in the Studio AI Lab, Prime Video & Amazon MGM Studios, London.
</div>

<div class="news-box">
  <span style="font-weight:600;">18/04/2024:</span> My 3-Minute Thesis, "How AI helps doctors diagnose swallowing difficulty," was selected as a <b>Semi-Finalist</b> in the 3MT competition at the University of Bristol.
  <div><img src="/images/portfolio/3MT.jpg" width="300"></div>
</div>

<div class="news-box">
  <span style="font-weight:600;">03/03/2024:</span> Our "RBF-PINN" paper was <b>accepted</b> by the International Conference on Learning Representations (<b>ICLR</b>), AI4DifferentialEquations (<b>AI4DE</b>) in Science workshop.
</div>

<div class="news-box">
  <span style="font-weight:600;">19/02/2024:</span> Our research proposal, "AI-aided diagnosis of Dysphagia in the Elderly Population," was chosen as one of the <b>Finalists</b> in the TakeAIM competition by the Smith Institute.
</div>

<div class="news-box">
  <span style="font-weight:600;">27/11/2023:</span> I co-organised an event for the Outstanding Young Scholars Society UK, led by
  <a href="https://warwick.ac.uk/fac/sci/eng/people/sai_gu/" target="_blank">Professor Sai Gu FREng</a>.
  <div><img src="/images/portfolio/Sai_Gu.jpg" width="300"></div>
</div>

</div>

---
## Projects
---
![Desktop View](/images/portfolio/C2D_loop.gif){: width="150" height="150" .w-10 .left}
[**C2D-ISR: Optimizing Attention-based Image Super-resolution from Continuous to Discrete Scales - 2025**](https://arxiv.org/abs/2503.13740) <br/>
Preprint, Under Review <br/>
Yuxuan Jiang, Chengxi Zeng, Siyue Teng, Fan Zhang, Xiaoqing Zhu, Joel Sole, David Bull <br/>
[[*arXiv*](https://arxiv.org/abs/2503.13740)]<br/>
In recent years, attention mechanisms have been exploited in single image super-resolution (SISR), achieving impressive reconstruction results. However, these advancements are still limited by the reliance on simple training strategies and network architectures designed for discrete up-sampling scales, which hinder the model's ability to effectively capture information across multiple scales. To address these limitations, we propose a novel framework, C2D-ISR, for optimizing attention-based image super-resolution models from both performance and complexity perspectives. Our approach is based on a two-stage training methodology and a hierarchical encoding mechanism. The new training methodology involves continuous-scale training for discrete scale models, enabling the learning of inter-scale correlations and multi-scale feature representation. In addition, we generalize the hierarchical encoding mechanism with existing attention-based network structures, which can achieve improved spatial feature fusion, cross-scale information aggregation, and more importantly, much faster inference. We have evaluated the C2D-ISR framework based on three efficient attention-based backbones, SwinIR-L, SRFormer-L and MambaIRv2-L, and demonstrated significant improvements over the other existing optimization framework, HiT, in terms of super-resolution performance (up to 0.2dB) and computational complexity reduction (up to 11%). 

---


![Desktop View](/images/portfolio/MT_Med.svg){: width="200" height="200" .w-10 .left}
[**Agglomerating Large Vision Encoders via Distillation for VFSS Segmentation - 2025**](https://arxiv.org/abs/2504.02351) <br/>
 IEEE/CVF Conference on Computer Vision and Pattern Recognition (**CVPR**) 2025, Efficient Large Vision Models Workshop (2nd Edition), **Accepted**<br/>
Chengxi Zeng, Yuxuan Jiang, Fan Zhang, Alberto Gambaruto, Tilo Burghardt <br/>


[[*arXiv*](https://arxiv.org/abs/2504.02351)]<br/>
The deployment of foundation models for medical imaging has demonstrated considerable success. However, their training overheads associated with downstream tasks remain substantial due to the size of the image encoders employed, and the inference complexity is also significantly high. Although lightweight variants have been obtained for these foundation models, their performance is constrained by their limited model capacity and suboptimal training strategies. In order to achieve an improved tradeoff between complexity and performance, we propose a new framework to improve the performance of low complexity models via knowledge distillation from multiple large medical foundation models (e.g., MedSAM, R50-DINO, MedCLIP). Each specializing in different vision tasks, with the goal to effectively bridge the performance gap for medical image segmentation tasks. The agglomerated model demonstrates superior generalization across 12 segmentation tasks, whereas specialized models require explicit training for each task. Our approach achieved an average performance gain of 2% in Dice coefficient compared to simple distillation.

---

![Desktop View](/images/portfolio/full_seg.gif){: width="150" height="150" .w-10 .left}
[**Tuning Vision Foundation Model via Test-Time Prompt-Guided Training for VFSS Segmentations - 2025**](https://arxiv.org/abs/2501.18474) <br/>
Preprint, Under Review <br/>
Chengxi Zeng, Tilo Burghardt, Alberto Gambaruto <br/>
[[*arXiv*](https://arxiv.org/abs/2501.18474)]<br/>
Vision foundation models have demonstrated exceptional generalization capabilities in segmentation tasks for both generic and specialized images. However, a performance gap persists between foundation models and task-specific, specialized models. Fine-tuning foundation models on downstream datasets is often necessary to bridge this gap. Unfortunately, obtaining fully annotated ground truth for downstream datasets is both challenging and costly. To address this limitation, we propose a novel test-time training paradigm that enhances the performance of foundation models on downstream datasets without requiring full annotations. Specifically, our method employs simple point prompts to guide a test-time semi-self-supervised training task. The model learns by resolving the ambiguity of the point prompt through various augmentations. This approach directly tackles challenges in the medical imaging field, where acquiring annotations is both time-intensive and expensive. We conducted extensive experiments on our new Videofluoroscopy dataset (VFSS-5k) for the instance segmentation task, achieving an average Dice coefficient of 0.868 across 12 anatomies with a single model.

---
![Desktop View](/images/portfolio/Yorkshiretea.gif){: width="150" height="150" .w-10 .left}
[**Tea Classification - 2024**](https://brewtone.ai/) <br/>
Chengxi Zeng, Ted Littledale, Yorkshire Tea (Bettys & Taylors of Harrogate) <br/>
[[*WEB APP*](https://brewtone.ai/)]<br/>
We developed a tea classification Web App for Yorkshire Tea (Bettys & Taylors of Harrogate). The app is able to classify tea colors from any cups. My main responsibility was to develop image processing code for the tea element and extract the main tone by applying various edge detection and outlier removal techniques. Additionally, I employed the segmentation foundation model [Segment Anything](https://segment-anything.com/)  on a small tea dataset I created. The model accurately segmented the tea element and was converted to an ONNX file that can be deployed to multiple platforms. Prior to UK national tea day, the app successfully attracted thousands of users in a few weeks.

---
![Desktop View](/images/portfolio/ns_fluid.gif){: width="150" height="150" .w-10 .left}
[**RBF-PINN: Non-Fourier Positional Embedding in Physics-Informed Neural Networks - 2024**](https://paperswithcode.com/paper/rbf-pinn-non-fourier-positional-embedding-in) <br/>
International Conference on Learning Representations (**ICLR 2024**) , AI4DifferentialEquations in Science Workshop, **Accepted**  <br/>
Chengxi Zeng, Tilo Burghardt, Alberto Gambaruto <br/>
[[*Github*](https://github.com/SimonZeng7108/RBF-PINN)] 
[[*arXiv*](https://arxiv.org/abs/2402.08367)]<br/>
While many recent Physics-Informed Neural Networks (PINNs) variants have had considerable success in solving Partial Differential Equations, the empirical benefits of feature mapping drawn from the broader Neural Representations research have been largely overlooked. We highlight the limitations of widely used Fourier-based feature mapping in certain situations and suggest the use of the conditionally positive definite Radial Basis Function. The empirical findings demonstrate the effectiveness of our approach across a variety of forward and inverse problem cases. Our method can be seamlessly integrated into coordinate-based input neural networks and contribute to the wider field of PINNs research.

---
![Desktop View](/images/portfolio/lorenz.gif){: width="200" height="200" .w-10 .left}
[**Training dynamics in Physics-Informed Neural Networks with feature mapping - 2024**](https://arxiv.org/abs/2402.06955) <br/>
Preprint, Under Review <br/>
Chengxi Zeng, Tilo Burghardt, Alberto Gambaruto <br/>
[[*Github*](https://github.com/SimonZeng7108/RBF-PINN)] 
[[*arXiv*](https://arxiv.org/abs/2402.06955)]<br/>
Physics-Informed Neural Networks (PINNs) have emerged as an iconic machine learning approach for solving Partial Differential Equations (PDEs). Although its variants have achieved significant progress, the empirical success of utilising feature mapping from the wider Implicit Neural Representations studies has been substantially neglected. We investigate the training dynamics of PINNs with a feature mapping layer via the limiting Conjugate Kernel and Neural Tangent Kernel, which sheds light on the convergence and generalisation of the model. We also show the inadequacy of commonly used Fourier-based feature mapping in some scenarios and propose the conditional positive definite Radial Basis Function as a better alternative. The empirical results reveal the efficacy of our method in diverse forward and inverse problem sets. This simple technique can be easily implemented in coordinate input networks and benefits the broad PINNs research.

---
![Desktop View](/images/portfolio/swin.gif){: width="150" height="150" .w-10 .left}
[**Video-SwinUNet: Spatio-Temporal deep learning framework for VFSS instance segmentation - 2023**](https://paperswithcode.com/paper/video-swinunet-spatio-temporal-deep-learning) <br/>
IEEE International Conference on Image Processing (**ICIP 2023**), **Accepted** <br/>
Chengxi Zeng, Xinyu Yang, David Smithard, Majid Mirmehdi, Alberto Gambaruto, Tilo Burghardt <br/>
[[*Github*](https://github.com/SimonZeng7108/Video-SwinUNet)] 
[[*arXiv*](https://arxiv.org/abs/2302.11325v1)]<br/>
This paper presents a deep learning framework for medical video segmentation. Our proposed framework explicitly extracts features from neighbouring frames across the temporal dimension and incorporates them with a novel temporal feature blender which then tokenises the high-level Spatio-temporal feature to a strong global feature encoder Swin Transformer. Our model outperforms other approaches by a significant margin and improves the segmentation benchmarks on the VFSS2022 dataset, achieving a dice coefficient of 0.8986/0.8186 for Part1/Part2 data. Our studies have also shown the efficacy of the temporal feature blending scheme and the transferability of the framework.

---
![Desktop View](/images/portfolio/arion.gif){: width="128" height="128" .w-10 .left}
[**Lithology Document Analysis - 2022**](https://www.cgg.com/sites/default/files/2022-02/2202_Lun_FB_ML%20Doc%20Extraction_art.pdf) <br/>
Chengxi Zeng, Arion.ai, CGG <br/>
We developed a deep learning pipeline that processes long lithology tracks to multi-page images, a fast bounding box detection algorithm is employed and calibrated. Segmentation models are used to extract the curves and hence the numerical data is restored. The pipeline is tested on 1000+ documents and can process 100+ Page/Sec

---
![Desktop View](/images/portfolio/swallowing.gif){: width="128" height="128" .w-10 .left}
[**Video-TransUNet: Temporally Blended Vision Transformer for CT VFSS Instance Segmentation - 2022**](https://deepai.org/publication/video-transunet-temporally-blended-vision-transformer-for-ct-vfss-instance-segmentation) <br/>
SPIE International Conference on Machine Vision (**ICMV 2022**), **Accepted**, **[Best Oral Presentation](/images/portfolio/ICMV_2022.png)**<br/>
Chengxi Zeng, Xinyu Yang, Majid Mirmehdi, Alberto Gambaruto, Tilo Burghardt <br/>
[[*Github*](https://github.com/SimonZeng7108/Video-TransUNet)] 
[[*arXiv*](https://arxiv.org/abs/2208.08315)]<br/>
We propose Video-TransUNet, a deep architecture for instance segmentation in medical CT videos constructed by integrating temporal feature blending into the TransUNet deep learning framework. In particular, our approach amalgamates strong frame representation via a ResNet CNN backbone, multi-frame feature blending via a Temporal Context Module (TCM), non-local attention via a Vision Transformer, and reconstructive capabilities for multiple targets via a UNet-based convolutional-deconvolutional architecture with multiple heads.

---
![Desktop View](/images/portfolio/drone.gif){: width="128" height="128" .w-10 .left}
[**Portable AED Delivery Emergency Drone (Patented) - 2022**](https://english.cnipa.gov.cn/) <br/>
Chengxi Zeng, Yuhang Ming, Mengxun Bai, Ruobing Li <br/>
We are developing an Emergency drone that delivers medical kits and portable AED which would save lives in jammed cities and remote areas. This project collaborates with Hangzhou Municipal Health Commission and the drone will work with local Emergency Response Unit.

---
![Desktop View](/images/portfolio/haisaimu.gif){: width="128" height="128" .w-10 .left}
[**Elastic Transformation Detection - 2021**](http://www.haytham.com.cn/qichejilingbujian.html) <br/>
Chengxi Zeng, Haytham Technology Ltd <br/>
We utilised CNN and other image processing techniques to non-destructively detect Elastic Transformation on steel/composite materials due to repetitive work load in extreme environments.

---
![Desktop View](/images/portfolio/lake.gif){: width="128" height="80" .w-10 .left}
[**Solving Partial Differential Equations using Radial Basis Functions - 2021**](/images/portfolio/Solving Partial Differential Equations using Radial Basis Function.pdf) <br/>
Chengxi Zeng, Andreas Michael, Celia Tugores-Bonilla, Natasha Moore, Jules Boisser, Alberto Gambaruto <br/>
We discretise standard Partial Differential Equations(PDEs) using Radial Basis Functions, aiming to assess their performance compared to the analytical results and alternative discretisation methods.

---
![Desktop View](/images/portfolio/citycfd.gif){: width="128" height="150" .w-10 .left}
[**Pedestrian Level wind assessment with CFD simulation - 2019**](http://www.bristol.ac.uk/engineering/departments/mecheng/courses/projects/#6) <br/>
Chengxi Zeng, Alberto Gambaruto <br/>
We modelled Bristol city centre area and applied CFD simulation with focus on high wind speed area and presented the advantage of trees to mitigate wind using Porous Media approach.




<br>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=250&t=n&d=hnXPU8X96e7s6ubaWzFOcgTks4PbHzmsEzYnZoEMVuE'></script>
